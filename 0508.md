```
2023-05-08 09:37:18 ERROR Executor:91 - Exception in task 6.0 in stage 168.0 (TID 5065)                                 org.apache.spark.api.python.PythonException: Traceback (most recent call last):                                           File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main                                                                                                                process()                                                                                                             File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process                                                                                                             serializer.dump_stream(func(split_index, iterator), outfile)                                                          File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 400, in dump_stream                                                                                                    vs = list(itertools.islice(iterator, batch))                                                                          File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/bigdl/orca/data/shard.py", line 228, in transform           yield func(x, *args)                                                                                                  File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/bigdl/orca/learn/utils.py", line 404, in to_shard_dict      for feature_col in feature_cols]                                                                                      File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/bigdl/orca/learn/utils.py", line 404, in <listcomp>         for feature_col in feature_cols]                                                                                      File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/bigdl/orca/learn/utils.py", line 391, in single_col_to_numpy                                                                                                                        dtype = col_series.iloc[0].dtype                                                                                    AttributeError: 'list' object has no attribute 'dtype'                                                                                                                                                                                                  at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)            at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)                                        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)                                        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)                          at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)                                       at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)                                       at org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:349)                                at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)                        at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)                        at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)                                                 at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)                                         at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)                                        at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)                                                                 at org.apache.spark.rdd.RDD.iterator(RDD.scala:308)                                                                     at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)                                                    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)                                                      at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)                                                                     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)                                                   at org.apache.spark.scheduler.Task.run(Task.scala:123)                                                                  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)                                  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)                                                    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)                                                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)                                      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)                                      at java.lang.Thread.run(Thread.java:750)                                                                        2023-05-08 09:37:18 WARN  BlockManager:66 - Putting block rdd_334_5 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):                                                                       File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main                                                                                                                process()                                                                                                             File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process                                                                                                             serializer.dump_stream(func(split_index, iterator), outfile)                                                          File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 400, in dump_stream                                                                                                    vs = list(itertools.islice(iterator, batch))                                                                          File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/bigdl/orca/data/shard.py", line 228, in transform           yield func(x, *args)                                                                                                  File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/bigdl/orca/learn/utils.py", line 404, in to_shard_dict      for feature_col in feature_cols]                                                                                      File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/bigdl/orca/learn/utils.py", line 404, in <listcomp>         for feature_col in feature_cols]                                                                                      File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/bigdl/orca/learn/utils.py", line 391, in single_col_to_numpy                                                                                                                        dtype = col_series.iloc[0].dtype                                                                                    AttributeError: 'list' object has no attribute 'dtype'                                                                  .                                                                                                                       2023-05-08 09:37:18 WARN  BlockManager:66 - Block rdd_334_5 could not be removed as it was not found on disk or in memory                                                                                                                       2023-05-08 09:37:18 WARN  TaskSetManager:66 - Lost task 2.0 in stage 168.0 (TID 5061, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):                                                File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main                                                                                                                process()                                                                                                             File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process                                                                                                             serializer.dump_stream(func(split_index, iterator), outfile)                                                          File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 400, in dump_stream                                                                                                    vs = list(itertools.islice(iterator, batch))                                                                          File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/bigdl/orca/data/shard.py", line 228, in transform           yield func(x, *args)                                                                                                  File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/bigdl/orca/learn/utils.py", line 404, in to_shard_dict      for feature_col in feature_cols]                                                                                      File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/bigdl/orca/learn/utils.py", line 404, in <listcomp>         for feature_col in feature_cols]                                                                                      File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/bigdl/orca/learn/utils.py", line 391, in single_col_to_numpy                                                                                                                        dtype = col_series.iloc[0].dtype                                                                                    AttributeError: 'list' object has no attribute 'dtype'                                                                                                                                                                                                  at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)            at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)                                        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)                                        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)                          at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)                                       at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)                                       at org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:349)                                at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)                        at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)                        at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)                                                 at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)                                         at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)                                        at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)                                                                 at org.apache.spark.rdd.RDD.iterator(RDD.scala:308)                                                                     at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)                                                    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)                                                      at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)                                                                     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)                                                   at org.apache.spark.scheduler.Task.run(Task.scala:123)                                                                  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)                                  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)                                                    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)                                                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)                                      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)                                      at java.lang.Thread.run(Thread.java:750)                                                                                                                                                                                                2023-05-08 09:37:18 ERROR Executor:91 - Exception in task 5.0 in stage 168.0 (TID 5064)                                 org.apache.spark.api.python.PythonException: Traceback (most recent call last):                                           File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main                                                                                                                process()                                                                                                             File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process                                                                                                             serializer.dump_stream(func(split_index, iterator), outfile)                                                          File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 400, in dump_stream                                                                                                    vs = list(itertools.islice(iterator, batch))                                                                          File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/bigdl/orca/data/shard.py", line 228, in transform           yield func(x, *args)                                                                                                  File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/bigdl/orca/learn/utils.py", line 404, in to_shard_dict      for feature_col in feature_cols]                                                                                      File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/bigdl/orca/learn/utils.py", line 404, in <listcomp>         for feature_col in feature_cols]                                                                                      File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/bigdl/orca/learn/utils.py", line 391, in single_col_to_numpy                                                                                                                        dtype = col_series.iloc[0].dtype                                                                                    AttributeError: 'list' object has no attribute 'dtype'                                                                                                                                                                                                  at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)            at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)                                        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)                                        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)                          at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)                                       at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)                                       at org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:349)                                at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)                        at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)                        at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)                                                 at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)                                         at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)                                        at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)                                                                 at org.apache.spark.rdd.RDD.iterator(RDD.scala:308)                                                                     at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)                                                    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)                                                      at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)                                                                     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)                                                   at org.apache.spark.scheduler.Task.run(Task.scala:123)                                                                  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)                                  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)                                                    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)                                                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)                                      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)                                      at java.lang.Thread.run(Thread.java:750)                                                                        2023-05-08 09:37:18 ERROR TaskSetManager:70 - Task 2 in stage 168.0 failed 1 times; aborting job                        Traceback (most recent call last):                                                                                        File "tf_train_xshards.py", line 91, in <module>                                                                          callbacks=callbacks)                                                                                                  File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/bigdl/orca/learn/tf2/ray_estimator.py", line 255, in fit    "fit")                                                                                                                File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/bigdl/orca/learn/utils.py", line 422, in process_xshards_of_pandas_dataframe                                                                                                        data = transform_to_shard_dict(data, feature_cols, label_cols)                                                        File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/bigdl/orca/learn/utils.py", line 415, in transform_to_shard_dict                                                                                                                    data = data.transform_shard(to_shard_dict)                                                                            File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/bigdl/orca/data/shard.py", line 230, in transform_shard     transformed_shard = self._create(self.rdd.mapPartitions(lambda iter:                                                  File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/bigdl/orca/data/shard.py", line 202, in _create             return SparkXShards(rdd, class_name=class_name)                                                                       File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/bigdl/orca/data/shard.py", line 176, in __init__            self.compute()                                                                                                        File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/bigdl/orca/data/shard.py", line 292, in compute             self.rdd.count()                                                                                                      File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/pyspark/rdd.py", line 1055, in count                        return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()                                                        File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/pyspark/rdd.py", line 1046, in sum                          return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)                                                   File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/pyspark/rdd.py", line 917, in fold                          vals = self.mapPartitions(func).collect()                                                                             File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/pyspark/rdd.py", line 816, in collect                       sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())                                                 File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/py4j/java_gateway.py", line 1257, in __call__               answer, self.gateway_client, self.target_id, self.name)                                                               File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/pyspark/sql/utils.py", line 63, in deco                     return f(*a, **kw)                                                                                                    File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value            format(target_id, ".", name), value)                                                                                py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 168.0 failed 1 times, most recent failure: Lost task 2.0 in stage 168.0 (TID 5061, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):                                                                                      File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main                                                                                                                process()                                                                                                             File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process                                                                                                             serializer.dump_stream(func(split_index, iterator), outfile)                                                          File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 400, in dump_stream                                                                                                    vs = list(itertools.islice(iterator, batch))                                                                          File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/bigdl/orca/data/shard.py", line 228, in transform           yield func(x, *args)                                                                                                  File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/bigdl/orca/learn/utils.py", line 404, in to_shard_dict      for feature_col in feature_cols]                                                                                      File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/bigdl/orca/learn/utils.py", line 404, in <listcomp>         for feature_col in feature_cols]                                                                                      File "/root/miniconda3/envs/py376/lib/python3.7/site-packages/bigdl/orca/learn/utils.py", line 391, in single_col_to_numpy                                                                                                                        dtype = col_series.iloc[0].dtype                                                                                    AttributeError: 'list' object has no attribute 'dtype'
```
